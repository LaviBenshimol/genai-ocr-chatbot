"""
Enhanced Grounded Answer Generation V2
Improved answer generation with fallback handling and better context formatting
"""
import os
import json
from typing import Dict, List, Any, Optional
from openai import AzureOpenAI
import logging

logger = logging.getLogger(__name__)


def _client() -> AzureOpenAI:
    return AzureOpenAI(
        azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT", ""),
        api_key=os.environ.get("AZURE_OPENAI_API_KEY", ""),
        api_version=os.environ.get("AZURE_OPENAI_API_VERSION") or "2024-10-21",
    )


def format_kb_context_for_llm(snippets: List[Dict[str, Any]]) -> str:
    """Format KB snippets into structured context for LLM"""
    
    if not snippets:
        return ""
    
    context_lines = ["注 专 住住 转:"]
    context_lines.append("=" * 50)
    
    for i, snippet in enumerate(snippets, 1):
        metadata = snippet.get("metadata", {})
        content = snippet.get("content", "")
        
        context_lines.append(f"\\n{i}. 拽专: {metadata.get('category', ' 专')}")
        if metadata.get('service'):
            context_lines.append(f"   砖专转: {metadata.get('service')}")
        if metadata.get('fund'):
            context_lines.append(f"   拽驻转 : {metadata.get('fund')}")
        if metadata.get('plan'):
            context_lines.append(f"   住: {metadata.get('plan')}")
        
        context_lines.append(f"   转: {content}")
        context_lines.append("-" * 30)
    
    return "\\n".join(context_lines)


def generate_grounded_answer_v2(
    user_question: str,
    user_profile: Dict[str, Any],
    kb_context: str,
    conversation_history: List[Dict[str, str]],
    language: str = "he",
    answer_type: str = "specific_benefits",
    category: str = "",
    fallback_used: bool = False,
    max_tokens: int = 1200
) -> Dict[str, Any]:
    """
    Generate enhanced grounded answer with better handling of different scenarios
    """
    
    hmo = user_profile.get('hmo', '')
    tier = user_profile.get('tier', '')
    
    # Build enhanced system prompt based on answer type and context
    if fallback_used:
        system_prompt = f"""
转 注专 注  专转 砖专  注.

砖: 住驻拽 注  注  转 转 拽专 "{category}"   爪 注 住驻爪驻.

转 砖转:
1. 住专 砖爪 注  注  驻砖专转 转 拽专
2. 专 转 注 爪专 专专 驻 拽驻转  住  
3. 砖   拽驻转  住 砖
4. 抓 拽 注 拽驻转  住驻爪驻转  驻专
5. 砖转砖 砖驻 专专 转
6.  砖 注 拽 , 爪 转 专专

驻专 转砖:
- 转 注 住专 拽爪专 砖爪 注 
- 专 驻 拽驻转  住  
- 住 注 爪 专专 住祝 注 拽驻转 

 驻转: " 注  注  转 转 转 {category}:"
"""
    else:
        system_prompt = f"""
转 注专 注  专转 砖专  注.

砖: 注 拽 注 住住 注 砖住驻拽 注 砖专转 专转.

转 砖转:
1. 砖转砖 专拽 注 砖驻注 拽砖专 砖住驻拽
2.  注 注专 {hmo} {tier} , 转拽 
3.   注 住驻爪驻, 爪 转 注  
4. 爪 驻专砖  注   住驻爪驻 住
5.  转爪 注 砖 驻注 拽砖专
6. 砖转砖 砖驻 专专 转
7.  爪 专专 住祝 注 拽驻转   专砖

驻专 转砖:
- 转砖 砖专 砖
- 驻专 转 住驻爪驻转  转  
- 住专 注 转 转  专
- 爪 专专 住祝  专砖
"""

    # Create conversation context
    recent_history = conversation_history[-4:] if conversation_history else []
    history_text = ""
    if recent_history:
        history_lines = []
        for turn in recent_history:
            role = "砖转砖" if turn.get("role") == "user" else "注专"
            content = turn.get("content", "")
            history_lines.append(f"{role}: {content}")
        history_text = "\\n".join(history_lines)

    # Build user message
    user_message_parts = [f"砖转 砖转砖: {user_question}"]
    
    if hmo or tier:
        profile_text = f"驻专驻 砖转砖: 拽驻转  {hmo if hmo else ' 专'}, 住 {tier if tier else ' 专'}"
        user_message_parts.append(profile_text)
    
    if history_text:
        user_message_parts.append(f"\\n住专转 砖:\\n{history_text}")
    
    user_message_parts.append(f"\\n注 专:\\n{kb_context}")
    
    if fallback_used:
        user_message_parts.append(f"\\n注专: 爪 注    爪 注 住驻爪驻 注专 驻专驻 拽砖.")

    user_message = "\\n\\n".join(user_message_parts)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

    try:
        client = _client()
        response = client.chat.completions.create(
            model=os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
            messages=messages,
            temperature=0.3,  # Slightly higher for more natural responses
            max_tokens=max_tokens,
            top_p=0.9,
            frequency_penalty=0.1,  # Slight penalty to avoid repetition
            presence_penalty=0.1    # Encourage diverse explanations
        )

        answer = response.choices[0].message.content or ""
        
        # Add fallback indicator if used
        if fallback_used and answer:
            if language == "he":
                answer = f" 注  :\\n\\n{answer}\\n\\n 拽转 注 拽 转专 注专 住 砖, 抓 驻转 砖专转 拽驻转  砖."
            else:
                answer = f" General information available:\\n\\n{answer}\\n\\n For more specific information about your plan, we recommend contacting your health fund directly."

        return {
            "answer": answer,
            "token_usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            "fallback_used": fallback_used,
            "answer_type": answer_type
        }

    except Exception as e:
        logger.error(f"Error generating grounded answer: {e}")
        
        # Fallback error response
        if language == "he":
            fallback_answer = "爪注专, 专注 砖 爪专转 转砖.  住 砖  驻 拽驻转  砖 砖专转."
        else:
            fallback_answer = "Sorry, an error occurred while generating the answer. Please try again or contact your health fund directly."
        
        return {
            "answer": fallback_answer,
            "token_usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            "fallback_used": True,
            "answer_type": "error",
            "error": str(e)
        }


def generate_collection_response(
    missing_fields: List[str],
    question_to_ask: str,
    alternative_offer: str,
    language: str = "he",
    politeness_level: str = "high"
) -> str:
    """
    Generate polite information collection response
    """
    
    if language == "he":
        if politeness_level == "high":
            response = f"砖! 砖 注专 . {question_to_ask}"
            if alternative_offer:
                response += f"\\n\\n{alternative_offer}"
            return response
        else:
            return question_to_ask
    else:
        if politeness_level == "high":
            response = f"Hello! I'd be happy to help you. {question_to_ask}"
            if alternative_offer:
                response += f"\\n\\n{alternative_offer}"
            return response
        else:
            return question_to_ask


def generate_scope_explanation(
    service_scope: str,
    available_services: List[str],
    language: str = "he"
) -> str:
    """
    Generate explanation when service is out of scope
    """
    
    if language == "he":
        if service_scope == "out_of_scope":
            services_text = ", ".join(available_services)
            return f"爪注专, 砖专转 砖拽砖转   注专转 注 砖. 砖专转  : {services_text}. 砖 注专   砖专转 ."
        else:
            return "砖 注专  注 砖专转  注专转."
    else:
        if service_scope == "out_of_scope":
            services_text = ", ".join(available_services)
            return f"Sorry, the service you requested is not available in our information system. Available services are: {services_text}. I'd be happy to help you with one of the available services."
        else:
            return "I'd be happy to help you with the available services in our system."
